{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b423de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc37b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, lr, block):\n",
    "    \"\"\"\n",
    "    training loop to train model, this is a standard training loop for resnet/DNN\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    log_values = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "        \n",
    "    app_epoch = f'-{num_epochs}e' if num_epochs != 10 else ''\n",
    "    \n",
    "    lr_code = str(lr).split('.')[-1]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('{}/{}'.format(epoch, num_epochs - 1), end='\\t')\n",
    "        # print('-' * 10)\n",
    "        log_value = []\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                # only eval at end trial\n",
    "                at_start = epoch == 0\n",
    "                at_end = epoch == num_epochs - 1\n",
    "                every_10 = ((epoch + 1) % 10) == 10\n",
    "                \n",
    "                to_eval = at_start or at_end or every_10\n",
    "                if not to_eval:\n",
    "                    continue # only eval once per 10 epochs\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # device = torch.device('cpu') if phase=='val' else torch.device('cuda')\n",
    "                device = torch.device('cuda')\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                model = model.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), end='\\t')\n",
    "            log_value += [epoch_loss, epoch_acc.item()]\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, f'{dataset_sz}-{lr_code}-{block}{app_epoch}-best-evalend.pt')\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        log_values.append(log_value)\n",
    "        pd.DataFrame(log_values).to_csv(f'{dataset_sz}-{lr_code}-{block}{app_epoch}-evalend.csv', index=False)\n",
    "        torch.save(model.state_dict(), f'{dataset_sz}-{lr_code}-{block}{app_epoch}-evalend.pt')\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    # TODO SAVE MODEL WEIGHTS AFTER RUN\n",
    "    return model, log_values, best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_fine_tune_block(model, block_num):\n",
    "    \"\"\"\n",
    "    Freeze/unfreeze specific blocks for fine-tune training\n",
    "    \"\"\"\n",
    "    # unfreeze all block for ALL\n",
    "    if block_num == \"ALL\":\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        return\n",
    "    # freeze all params\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # unfreeze predefined block\n",
    "    block = eval(f'model.layer{block_num}')\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b84477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Might need to adjust for different data sets\n",
    "DATA_DIR = \"/mnt/d/dataset/stylized-imagenet-10k\"\n",
    "# available DATA_DIRs\n",
    "# whole dataset: /mnt/d/dataset/stylized-imagenet\n",
    "# subsets:\n",
    "# /home/rs/stylized-imagenet-1k\n",
    "# /home/rs/stylized-imagenet-5k\n",
    "# /home/rs/stylized-imagenet-10k\n",
    "\n",
    "# DATA_DIR = \"/home/rs/stylized-imagenet-1k\"\n",
    "data_dir = DATA_DIR\n",
    "\n",
    "# dataset_sz = DATA_DIR.split('-')[-1]\n",
    "\n",
    "# May need to adjust per machine specs\n",
    "# 128 uses ~14GB, 192 uses 19,5 GB, 256 uses 24GB. 192 ~= 256\n",
    "BATCH_SIZE = 192 \n",
    "\n",
    "def check_valid(path):\n",
    "    path = Path(path)\n",
    "    return not path.stem.startswith('._')\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256), # TODO imagenet training uses 256 input size, crop to 224 on the next line\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(\n",
    "    os.path.join(data_dir, x), \n",
    "    data_transforms[x],\n",
    "    is_valid_file=check_valid\n",
    ") for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                   batch_size=BATCH_SIZE, \n",
    "                                                   shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1757d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(LEARNING_RATE, FINE_TUNE_BLOCK,NUM_EPOCHS=10):\n",
    "    print(f\"{lr=}, {FINE_TUNE_BLOCK=}\")\n",
    "    \n",
    "    # initialize fresh model from resnet pretrained\n",
    "    model_ft = models.resnet50(weights='ResNet50_Weights.IMAGENET1K_V1')\n",
    "    \n",
    "    # freeze/unfreeze specific block\n",
    "    only_fine_tune_block(model_ft, FINE_TUNE_BLOCK)\n",
    "\n",
    "    model_ft = model_ft.to(device)\n",
    "    # Gather parameters to be optimized by checking whether requires_grad is True\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    model_ft, hist, best_model_wt = train_model(\n",
    "        model_ft,\n",
    "        dataloaders_dict,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        block=FINE_TUNE_BLOCK\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all combination except for data count (set through DATA_DIR)\n",
    "LRS = [\n",
    "    0.001,\n",
    "    0.0001,\n",
    "    0.00001\n",
    "]\n",
    "\n",
    "FINE_TUNE_BLOCKS = [\n",
    "    \"ALL\",\n",
    "    1,2,3,4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb15848-8853-4915-882a-023183a65a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in LRS:\n",
    "    for block in FINE_TUNE_BLOCKS:\n",
    "        finetune_model(lr, block) # add num_epochs, try with 1 first\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282295d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
