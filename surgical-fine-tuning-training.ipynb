{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b423de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.1+cu117\n",
      "Torchvision Version:  0.14.1+cu117\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc37b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, lr, block):\n",
    "    \"\"\"\n",
    "    training loop to train model, this is a standard training loop for resnet/DNN\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    log_values = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "        \n",
    "    app_epoch = f'-{num_epochs}e' if num_epochs != 10 else ''\n",
    "    \n",
    "    lr_code = str(lr).split('.')[-1]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('{}/{}'.format(epoch, num_epochs - 1), end='\\t')\n",
    "        # print('-' * 10)\n",
    "        log_value = []\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                # only eval at end trial\n",
    "                at_start = epoch == 0\n",
    "                at_end = epoch == num_epochs - 1\n",
    "                every_10 = ((epoch + 1) % 10) == 10\n",
    "                \n",
    "                to_eval = at_start or at_end or every_10\n",
    "                if not to_eval:\n",
    "                    continue # only eval once per 10 epochs\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), end='\\t')\n",
    "            log_value += [epoch_loss, epoch_acc.item()]\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, f'{dataset_sz}-{lr_code}-{block}{app_epoch}-best-evalend.pt')\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        log_values.append(log_value)\n",
    "        pd.DataFrame(log_values).to_csv(f'{dataset_sz}-{lr_code}-{block}{app_epoch}-evalend.csv', index=False)\n",
    "        torch.save(model.state_dict(), f'{dataset_sz}-{lr_code}-{block}{app_epoch}-evalend.pt')\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    # TODO SAVE MODEL WEIGHTS AFTER RUN\n",
    "    return model, log_values, best_model_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_fine_tune_block(model, block_num):\n",
    "    \"\"\"\n",
    "    Freeze/unfreeze specific blocks for fine-tune training\n",
    "    \"\"\"\n",
    "    # freeze all params\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # unfreeze predefined block\n",
    "    block = eval(f'model.layer{block_num}')\n",
    "    for param in block.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b84477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Might need to adjust for different data sets\n",
    "# DATA_DIR = \"/mnt/d/dataset/stylized-imagenet\"\n",
    "# available DATA_DIRs\n",
    "# whole dataset: /mnt/d/dataset/stylized-imagenet\n",
    "# subsets:\n",
    "# /home/rs/stylized-imagenet-5k\n",
    "# /home/rs/stylized-imagenet-1k\n",
    "# /home/rs/stylized-imagenet-10k\n",
    "\n",
    "DATA_DIR = \"/home/rs/stylized-imagenet-1k\"\n",
    "data_dir = DATA_DIR\n",
    "\n",
    "dataset_sz = DATA_DIR.split('-')[-1]\n",
    "\n",
    "# May need to adjust per machine specs\n",
    "# 128 uses ~14GB, 192 uses 19,5 GB, 256 uses 24GB. 192 ~= 256\n",
    "BATCH_SIZE = 192 \n",
    "\n",
    "def check_valid(path):\n",
    "    path = Path(path)\n",
    "    return not path.stem.startswith('._')\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256), # TODO imagenet training uses 256 input size, crop to 224 on the next line\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(\n",
    "    os.path.join(data_dir, x), \n",
    "    data_transforms[x],\n",
    "    is_valid_file=check_valid\n",
    ") for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                   batch_size=BATCH_SIZE, \n",
    "                                                   shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1757d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(LEARNING_RATE, FINE_TUNE_BLOCK,NUM_EPOCHS=10):\n",
    "    print(f\"{lr=}, {FINE_TUNE_BLOCK=}\")\n",
    "    \n",
    "    # initialize fresh model from resnet pretrained\n",
    "    model_ft = models.resnet50(weights='ResNet50_Weights.IMAGENET1K_V1')\n",
    "    \n",
    "    # freeze/unfreeze specific block\n",
    "    only_fine_tune_block(model_ft, FINE_TUNE_BLOCK)\n",
    "\n",
    "    model_ft = model_ft.to(device)\n",
    "    # Gather parameters to be optimized by checking whether requires_grad is True\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    model_ft, hist, best_model_wt = train_model(\n",
    "        model_ft,\n",
    "        dataloaders_dict,\n",
    "        criterion,\n",
    "        optimizer_ft,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        block=FINE_TUNE_BLOCK\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all combination except for data count (set through DATA_DIR)\n",
    "LRS = [\n",
    "    # 0.001,\n",
    "    # 0.0001,\n",
    "    0.00001\n",
    "]\n",
    "\n",
    "FINE_TUNE_BLOCKS = [\n",
    "    3,\n",
    "    2,\n",
    "    1,\n",
    "    4,\n",
    "    \n",
    "    # 4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df101731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=1e-05, FINE_TUNE_BLOCK=3\n",
      "0/9\ttrain Loss: 5.4997 Acc: 0.1390\tval Loss: 4.6562 Acc: 0.2309\t\n",
      "1/9\ttrain Loss: 5.5483 Acc: 0.1420\t\n",
      "2/9\ttrain Loss: 5.3029 Acc: 0.1600\t\n",
      "3/9\ttrain Loss: 5.4090 Acc: 0.1380\t\n",
      "4/9\ttrain Loss: 5.5113 Acc: 0.1410\t\n",
      "5/9\ttrain Loss: 5.5583 Acc: 0.1550\t\n",
      "6/9\ttrain Loss: 5.5341 Acc: 0.1450\t\n",
      "7/9\ttrain Loss: 5.4032 Acc: 0.1560\t\n",
      "8/9\ttrain Loss: 5.4245 Acc: 0.1460\t\n",
      "9/9\ttrain Loss: 5.4020 Acc: 0.1500\tval Loss: 4.6541 Acc: 0.2243\t\n",
      "Training complete in 3m 29s\n",
      "Best val Acc: 0.230863\n",
      "lr=1e-05, FINE_TUNE_BLOCK=2\n",
      "0/9\ttrain Loss: 5.6223 Acc: 0.1340\tval Loss: 4.6519 Acc: 0.2307\t\n",
      "1/9\ttrain Loss: 5.5447 Acc: 0.1490\t\n",
      "2/9\ttrain Loss: 5.4428 Acc: 0.1540\t\n",
      "3/9\ttrain Loss: 5.4856 Acc: 0.1510\t\n",
      "4/9\ttrain Loss: 5.4867 Acc: 0.1510\t\n",
      "5/9\ttrain Loss: 5.4328 Acc: 0.1740\t\n",
      "6/9\ttrain Loss: 5.4402 Acc: 0.1540\t\n",
      "7/9\ttrain Loss: 5.4658 Acc: 0.1620\t\n",
      "8/9\ttrain Loss: 5.4260 Acc: 0.1500\t\n",
      "9/9\ttrain Loss: 5.4264 Acc: 0.1600\tval Loss: 4.6646 Acc: 0.2244\t\n",
      "Training complete in 3m 29s\n",
      "Best val Acc: 0.230663\n",
      "lr=1e-05, FINE_TUNE_BLOCK=1\n",
      "0/9\ttrain Loss: 5.4951 Acc: 0.1560\tval Loss: 4.6453 Acc: 0.2315\t\n",
      "1/9\ttrain Loss: 5.3732 Acc: 0.1570\t\n",
      "2/9\ttrain Loss: 5.4373 Acc: 0.1550\t\n",
      "3/9\ttrain Loss: 5.3084 Acc: 0.1580\t\n",
      "4/9\ttrain Loss: 5.4260 Acc: 0.1600\t\n",
      "5/9\ttrain Loss: 5.4609 Acc: 0.1560\t\n",
      "6/9\ttrain Loss: 5.4933 Acc: 0.1470\t\n",
      "7/9\ttrain Loss: 5.3991 Acc: 0.1460\t\n",
      "8/9\ttrain Loss: 5.4707 Acc: 0.1470\t\n",
      "9/9\ttrain Loss: 5.4627 Acc: 0.1490\tval Loss: 4.6468 Acc: 0.2312\t\n",
      "1/9\ttrain Loss: 5.5456 Acc: 0.1500\t\n",
      "2/9\ttrain Loss: 5.4376 Acc: 0.1580\t\n",
      "3/9\ttrain Loss: 5.5255 Acc: 0.1500\t\n",
      "4/9\ttrain Loss: 5.5104 Acc: 0.1740\t\n",
      "5/9\ttrain Loss: 5.4362 Acc: 0.1640\t\n",
      "6/9\ttrain Loss: 5.3767 Acc: 0.1660\t\n",
      "7/9\ttrain Loss: 5.4155 Acc: 0.1580\t\n",
      "8/9\ttrain Loss: 5.3320 Acc: 0.1740\t\n",
      "9/9\ttrain Loss: 5.4088 Acc: 0.1460\tval Loss: 4.6674 Acc: 0.2232\t\n",
      "Training complete in 3m 26s\n",
      "Best val Acc: 0.231243\n"
     ]
    }
   ],
   "source": [
    "for lr in LRS:\n",
    "    for block in FINE_TUNE_BLOCKS:\n",
    "        finetune_model(lr, block) # add num_epochs, try with 1 first\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccc746",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.001, FINE_TUNE_BLOCK=2\n",
      "0/99\ttrain Loss: 5.2342 Acc: 0.1717\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m ll \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# (0.02,3,1), # trial throwaway\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# (0.01, 3, 10),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     (\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr, block, epochs \u001b[38;5;129;01min\u001b[39;00m ll:\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mfinetune_model\u001b[0;34m(LEARNING_RATE, FINE_TUNE_BLOCK, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     20\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model_ft, hist, best_model_wt \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFINE_TUNE_BLOCK\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, lr, block)\u001b[0m\n\u001b[1;32m     50\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# statistics\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     56\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloaders[phase]\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ll = [\n",
    "    # (0.02,3,1), # trial throwaway\n",
    "    # (0.01, 3, 10),\n",
    "    # (0.1, 3, 10), \n",
    "    (0.001, 2, 100),\n",
    "    (0.001, 4, 100),\n",
    "    (0.001, 1, 100)\n",
    "]\n",
    "\n",
    "for lr, block, epochs in ll:\n",
    "    finetune_model(lr, block, epochs)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4a1e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rs/miniconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 5.3302 Acc: 0.1663\n",
      "val Loss: 4.4713 Acc: 0.2443\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 5.1819 Acc: 0.1751\n",
      "val Loss: 4.3010 Acc: 0.2585\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 5.0858 Acc: 0.1849\n",
      "val Loss: 4.2338 Acc: 0.2625\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 5.0084 Acc: 0.1833\n",
      "val Loss: 4.1818 Acc: 0.2674\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 4.9937 Acc: 0.1817\n",
      "val Loss: 4.1209 Acc: 0.2732\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 4.9473 Acc: 0.1858\n",
      "val Loss: 4.0721 Acc: 0.2773\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 4.9247 Acc: 0.1874\n",
      "val Loss: 4.0423 Acc: 0.2795\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 4.8484 Acc: 0.1984\n",
      "val Loss: 4.0402 Acc: 0.2791\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 4.8369 Acc: 0.2037\n",
      "val Loss: 4.0046 Acc: 0.2819\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 4.8314 Acc: 0.1990\n",
      "val Loss: 3.9704 Acc: 0.2833\n",
      "\n",
      "Training complete in 19m 21s\n",
      "Best val Acc: 0.283288\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m LRS:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m FINE_TUNE_BLOCKS:\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mfinetune_model\u001b[0;34m(LEARNING_RATE, FINE_TUNE_BLOCK)\u001b[0m\n\u001b[1;32m     24\u001b[0m model_ft, hist, best_model_wt \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     25\u001b[0m     model_ft,\n\u001b[1;32m     26\u001b[0m     dataloaders_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m lr_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(LEARNING_RATE)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 33\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(hist)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10k-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINE_TUNE_BLOCK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model_ft\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10k-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINE_TUNE_BLOCK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model_wt, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10k-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINE_TUNE_BLOCK\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-best.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "for lr in LRS:\n",
    "    for block in FINE_TUNE_BLOCKS:\n",
    "        finetune_model(lr, block)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670b52f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
